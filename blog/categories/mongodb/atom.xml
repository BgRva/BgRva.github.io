<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Mongodb | BgRva]]></title>
  <link href="http://bgrva.github.io/blog/categories/mongodb/atom.xml" rel="self"/>
  <link href="http://bgrva.github.io/"/>
  <updated>2015-05-20T21:10:56-04:00</updated>
  <id>http://bgrva.github.io/</id>
  <author>
    <name><![CDATA[Michael Pastore]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Deploy Crawler to EC2 With Scrapyd]]></title>
    <link href="http://bgrva.github.io/blog/2014/04/13/deploy-crawler-to-ec2-with-scrapyd/"/>
    <updated>2014-04-13T21:21:01-04:00</updated>
    <id>http://bgrva.github.io/blog/2014/04/13/deploy-crawler-to-ec2-with-scrapyd</id>
    <content type="html"><![CDATA[<p>This post will walk through using <a href="http://scrapyd.readthedocs.org/en/latest/">scrapyd</a> to deploy and manage a <a href="http://scrapy.org/">scrapy</a> spider from a local linux instance to a remote EC2 linux instance. The steps below were run on a local Ubuntu 12.04 instance any one of the free AWS linux Ubuntu 12.04 tiers. This is another step in the development of the scraping project described in the first post for which the indented goal is to have multiple instances of scrapy spiders crawling for extended periods of time and saving the items to a common database for further processing.</p>

<p>This post assumes you have scrapy and scrapyd installed on a local (linux) system, a working scrapy crawler, an AWS account, and have some knowledge of basic web security practices. Note: If you have not yet installed scrapy on your local system, when installing pip for scrapy, install the ‘setup tools’ and not the ‘distro tools’, as the setup tools will be needed by scrapyd for ‘egg-i-fying’ crawlers for deployment.</p>

<p>If you don’t have an AWS then get one set up first and spend some time reading about <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/IAMBestPractices.html">IAM best practices</a>. In particular, don’t use your main account credentials for development. Create a <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/IAMBestPractices.html#create-iam-users">new user</a> in your AWS account and assign that user admin rights and do all development with that user. This separates you development credentials from your master account credentials (which have your credit card).</p>

<h3>Setting up scrapy on a EC2 Ubuntu instance</h3>

<p>These are the broad steps I took to get an Ubuntu Server 12.04 LTS (free tier) instance up and running with scrapy and scrapyd installed. They do not cover all the details. If this is your first time with AWS, there are plenty of docs and quality videos available – invest some time in them.</p>

<p>1) Log into your AWS account</p>

<p>2) Go to your EC2 Dashboard</p>

<p>3) Create a Security Group with the following inbound and outbound rules:</p>

<p>Inbound (for inbound ssh and scrapyd communication):
<img src="/images/ec2-inbound-rules.jpg" title="&lsquo;inbound rules&rsquo; &lsquo;image of ec2 inbound rules&rsquo;" ></p>

<p>Outbound (for outbound scrapyd and crawler http gets):
<img src="/images/ec2-outbound-rules.jpg" title="&lsquo;outbound rules&rsquo; &lsquo;image of ec2 outbound rules&rsquo;" ></p>

<p>4) Create a key value pair for the ‘dev’ user and download it so you have access to it on your local system</p>

<p>5) Launch a linux instance and associate the security group and key value pair with this instance, (we will use my-ec2.amazonaws.com as the public dns of the instance)</p>

<p>6) When the instance is running, connect to the EC2 instance using the key-value pair, <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AccessingInstancesLinux.html">connect to your instance</a></p>

<p>8) Install scrapy and scrapyd on the EC2 instance</p>

<h3>Prepare the crawler for deployment</h3>

<p>Scrapyd deployment <a href="http://scrapyd.readthedocs.org/en/latest/deploy.html#show-and-define-targets">targets</a> are specified in a crawler’s <em>scrapy.cfg</em> file. The scrapyd commands to deploy a crawler need to be run in the root directory of your crawler project (or use fully qualified paths).</p>

<p>1) On your local system, go to the root folder of your crawler project</p>

<p>2) Edit <em>scrapy.cfg</em> by replacing any <em>deploy</em> sections so that the file looks like the following:</p>

<p><figure class='code'><figcaption><span>scrapy.cfg  </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">h1</span><span class="o">&gt;</span><span class="n">Automatically</span> <span class="n">created</span> <span class="n">by</span><span class="p">:</span> <span class="n">scrapy</span> <span class="n">startproject</span><span class="o">&lt;/</span><span class="n">h1</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="c">#&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">h1</span><span class="o">&gt;</span><span class="n">For</span> <span class="n">more</span> <span class="n">information</span> <span class="n">about</span> <span class="n">the</span> <span class="p">[</span><span class="n">deploy</span><span class="p">]</span> <span class="n">section</span> <span class="n">see</span><span class="p">:</span><span class="o">&lt;/</span><span class="n">h1</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">h1</span><span class="o">&gt;&lt;</span><span class="n">a</span> <span class="n">href</span><span class="o">=</span><span class="s">&quot;http://doc.scrapy.org/en/latest/topics/scrapyd.html&quot;</span><span class="o">&gt;</span><span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">doc</span><span class="o">.</span><span class="n">scrapy</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">en</span><span class="o">/</span><span class="n">latest</span><span class="o">/</span><span class="n">topics</span><span class="o">/</span><span class="n">scrapyd</span><span class="o">.</span><span class="n">html</span><span class="o">&lt;/</span><span class="n">a</span><span class="o">&gt;&lt;/</span><span class="n">h1</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="p">[</span><span class="n">settings</span><span class="p">]</span>
</span><span class='line'><span class="n">default</span> <span class="o">=</span> <span class="n">projectX</span><span class="o">.</span><span class="n">settings</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="p">[</span><span class="n">deploy</span><span class="p">:</span><span class="n">local</span><span class="o">-</span><span class="n">target</span><span class="p">]</span>
</span><span class='line'><span class="n">url</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">a</span> <span class="n">href</span><span class="o">=</span><span class="s">&quot;http://localhost:6800/&quot;</span><span class="o">&gt;</span><span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">localhost</span><span class="p">:</span><span class="mi">6800</span><span class="o">/&lt;/</span><span class="n">a</span><span class="o">&gt;</span>
</span><span class='line'><span class="n">project</span> <span class="o">=</span> <span class="n">projectX</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="p">[</span><span class="n">deploy</span><span class="p">:</span><span class="n">aws</span><span class="o">-</span><span class="n">target</span><span class="p">]</span>
</span><span class='line'><span class="n">url</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">a</span> <span class="n">href</span><span class="o">=</span><span class="s">&quot;http://my-ec2.amazonaws.com:6800/&quot;</span><span class="o">&gt;</span><span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">my</span><span class="o">-</span><span class="n">ec2</span><span class="o">.</span><span class="n">amazonaws</span><span class="o">.</span><span class="n">com</span><span class="p">:</span><span class="mi">6800</span><span class="o">/&lt;/</span><span class="n">a</span><span class="o">&gt;</span>
</span><span class='line'><span class="n">project</span> <span class="o">=</span> <span class="n">projectX</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>Here we have added two scrapyd deployment targets, one to localhost and one to the EC2 instance.</p>

<p>3) Verify the targets with the command:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scrapyd-deploy -l</span></code></pre></td></tr></table></div></figure></p>

<p>you should see</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>local-target           &lt;a href="http://localhost:6800/">http://localhost:6800/&lt;/a>
</span><span class='line'>aws-target             &lt;a href="http://my-ec2.amazonaws.com:6800/">http://my-ec2.amazonaws.com:6800/&lt;/a></span></code></pre></td></tr></table></div></figure></p>

<h3>Deploy the crawler to EC2</h3>

<p>Make sure scrapyd is running on the target instance. For our EC2 instance we can check this by using one of the scrapy web service commands or in a browser navigate to the scrapyd web page (format: my-ec2.amazonaws.com:6800). Note: after I had scrapyd installed on EC2, if I stopped and restarted the instance, scrapyd would be run on startup. When I tried to run scrapyd I got an error “port 6800 in use”. If you get a similar error check if scrapyd is already running.</p>

<p>1) To deploy the spider to the EC2 target use the scrapyd command:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scrapyd-deploy aws-target -p projectX</span></code></pre></td></tr></table></div></figure></p>

<p>you should see:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Packing version 1396800856  &lt;br/>
</span><span class='line'>    Deploying to project &ldquo;projectX&rdquo; in &lt;a href="http://my-ec2.amazonaws.com:6800/addversion.json">http://my-ec2.amazonaws.com:6800/addversion.json&lt;/a>
</span><span class='line'>    Server response (200):
</span><span class='line'>    {&ldquo;status&rdquo;: &ldquo;ok&rdquo;, &ldquo;project&rdquo;: &ldquo;projectX&rdquo;, &ldquo;version&rdquo;: &ldquo;1396800856&rdquo;, &ldquo;spiders&rdquo;:1}</span></code></pre></td></tr></table></div></figure></p>

<p>2) Verify the deployment using the scrapyd web service:</p>

<p><figure class='code'><figcaption><span>scrapyd command  </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$> curl &lt;a href="http://my-ec2.amazonaws.com:6800/listprojects.json">http://my-ec2.amazonaws.com:6800/listprojects.json&lt;/a></span></code></pre></td></tr></table></div></figure></p>

<p>you should see:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{&ldquo;status&rdquo;: ok, &ldquo;projects&rdquo;: [&ldquo;projectX&rdquo;]}</span></code></pre></td></tr></table></div></figure></p>

<p>or you can refresh the scrapyd webpage on your remote instance and Available projects</p>

<h3>Schedule and instance of the crawler</h3>

<p>1) Use the scrapyd web service to schedule to the spider. Note: for the spiders I needed to run, each had multiple constructor parameters. To schedule a spider with constructor parameters, each parameter must be preceded by -d and they must be in the order as they appear in the spider constructor. For this example, the constructor parameters are <em>session_id</em>, <em>seed_id</em>, and <em>seed_url</em>. The particular spider to be used is <em>spider2b</em>:</p>

<p><figure class='code'><figcaption><span>scrapyd command </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl &lt;a href="http://my-ec2.amazonaws.com:6800/schedule.json">http://my-ec2.amazonaws.com:6800/schedule.json&lt;/a> -d project=projectX -d spider=spider2b -d session_id=33 -d seed_id=54 -d seed_url=&lt;a href="http://www.blah.com/">http://www.blah.com/&lt;/a></span></code></pre></td></tr></table></div></figure></p>

<p>you should see a response similar to:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{&ldquo;status&rdquo;: &ldquo;ok&rdquo;, &ldquo;jobid&rdquo;: &ldquo;c77c633ac28011e3ac0a0a32be087ede&rdquo;}</span></code></pre></td></tr></table></div></figure></p>

<p>2) Verify the job:</p>

<p><figure class='code'><figcaption><span>scrapyd command </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl &lt;a href="http://my-ec2.amazonaws.com:6800/listjobs.json?project=projectX">http://my-ec2.amazonaws.com:6800/listjobs.json?project=projectX&lt;/a></span></code></pre></td></tr></table></div></figure></p>

<p>will show:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{&ldquo;status&rdquo;: &ldquo;ok&rdquo;, &ldquo;running&rdquo;: [{&ldquo;id&rdquo;: &ldquo;c77c633ac28011e3ac0a0a32be087ede&rdquo;, &ldquo;spider&rdquo;: &ldquo;spider2b&rdquo;}], &ldquo;finished&rdquo;: [], &ldquo;pending&rdquo;: []}</span></code></pre></td></tr></table></div></figure></p>

<p>or you can refresh the scrapyd webpage on your remote instance and check Jobs</p>

<p>3) To stop a job, use the following</p>

<p><figure class='code'><figcaption><span>scrapyd command </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl &lt;a href="http://my-ec2.amazonaws.com:6800/cancel.json">http://my-ec2.amazonaws.com:6800/cancel.json&lt;/a> -d project=simple -d job=c77c633ac28011e3ac0a0a32be087ede</span></code></pre></td></tr></table></div></figure></p>

<p>you should see:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{&ldquo;status&rdquo;: &ldquo;ok&rdquo;, &ldquo;prevstate&rdquo;: &ldquo;running&rdquo;}</span></code></pre></td></tr></table></div></figure></p>

<p>Note that the <em>schedule</em> and <em>cancel</em> commands of the scrapyd webservice are not immediate, so give them a bit to take affect. When getting started, the scrapyd web page on the EC2 instance is the easiest way of viewing the logs and output items.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scrapy to MongoDB]]></title>
    <link href="http://bgrva.github.io/blog/2014/03/23/scraping-to-mongodb/"/>
    <updated>2014-03-23T20:11:01-04:00</updated>
    <id>http://bgrva.github.io/blog/2014/03/23/scraping-to-mongodb</id>
    <content type="html"><![CDATA[<p>Now that we can crawl some sites (based on the previous posts), we need to persist the data somewhere where it can be retrieved and processed for analysis. Recall that the goal of the scraper was to harvest connections between domains in order to generate a model of a specific industry’s on-line presence. In Scrapy, <a href="http://doc.scrapy.org/en/latest/topics/item-pipeline.html">Item Pipelines</a> are the prescribed mechanism by which scraped items can be persisted to a data store. In this post, we will be using a custom pipeline extension for <a href="http://www.mongodb.org/">MongoDB</a> which we will further customize to check for duplicates. The code examples in this post build upon those in the previous examples. It is also assumed that you have some familiarity with MongoDB and have a working (basic) crawl spider. It does not cover installing Scrapy, MongoDB, or any dependencies, (there is plenty of good documentation on these subjects). Before continuing, you will need to install the following (in addition to having Scrapy working):</p>

<p>– <a href="http://www.mongodb.org/">MongoDB</a><br/>
– <a href="https://github.com/sebdah/scrapy-mongodb">Scrapy-MongoDB</a>, an item pipeline extension written by Sebastian Dahlgren<br/></p>

<p>Once installed, the first step will be to get Scrapy-MongoDB working and saving to a collection. Once you’ve got MongoDB installed, create a database named scrapy and within it, a collection named items. You can use the crawl spider from the previous posts and update the settings.py file to use Scrapy-MongoDB. A quick note about MongoDB ids: mongo will automatically add a unique id object with each item if and id field is not specified. We will use default id and benefit from it because the default MongoDB id object contains an embedded <a href="http://docs.mongodb.org/manual/reference/method/ObjectId.getTimestamp/#ObjectId.getTimestamp">date-time stamp</a>. Once you get Scrapy-MongoDB working and you are saving to a collection, we need to extend the <a href="https://github.com/sebdah/scrapy-mongodb/blob/master/scrapy_mongodb.py">MongoDBPipeline</a> class to include behavior to check for duplicates. A duplicate of the example item is one in which the following fields match:</p>

<p>– Session_id<br/>
– Current_url<br/>
– Referring_url<br/></p>

<p>The <em>session_id</em> is used to group items from different harvests. We want to persist a single connection between two given urls for each harvest session. The <em>current_url</em> and <em>referring_url</em> will be used to represent the connection between two domains and will be used to generate a directed graph for the model, (a connection between two valid business backed domains can be used to infer a relationship which in turn can be used in social network analysis, more on this later …).</p>

<p>The new class will be called <em>CustomMongoDBPipeline</em> and should be placed within the <em>pipelines.py</em> file in the scrapy project folder. You can keep the default pipeline initialized with the project in the same file and switch back by changing the settings file.</p>

<p><figure class='code'><figcaption><span>pipelines.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">scrapy.exceptions</span> <span class="kn">import</span> <span class="n">DropItem</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">scrapy_mongodb</span> <span class="kn">import</span> <span class="n">MongoDBPipeline</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="k">class</span> <span class="nc">CustomMongoDBPipeline</span><span class="p">(</span><span class="n">MongoDBPipeline</span><span class="p">):</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
</span><span class='line'>    <span class="c"># the following lines are a duplication of MongoDBPipeline.process_item()</span>
</span><span class='line'>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s">&#39;buffer&#39;</span><span class="p">]:</span>
</span><span class='line'>        <span class="bp">self</span><span class="o">.</span><span class="n">current_item</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class='line'>        <span class="n">item</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s">&#39;append_timestamp&#39;</span><span class="p">]:</span>
</span><span class='line'>            <span class="n">item</span><span class="p">[</span><span class="s">&#39;scrapy-mongodb&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span> <span class="s">&#39;ts&#39;</span><span class="p">:</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">utcnow</span><span class="p">()</span> <span class="p">}</span>
</span><span class='line'>
</span><span class='line'>        <span class="bp">self</span><span class="o">.</span><span class="n">item_buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_item</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s">&#39;buffer&#39;</span><span class="p">]:</span>
</span><span class='line'>            <span class="bp">self</span><span class="o">.</span><span class="n">current_item</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class='line'>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">insert_item</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">item_buffer</span><span class="p">,</span> <span class="n">spider</span><span class="p">)</span>
</span><span class='line'>        <span class="k">else</span><span class="p">:</span>
</span><span class='line'>            <span class="k">return</span> <span class="n">item</span>
</span><span class='line'>
</span><span class='line'>    <span class="c"># if the connection exists, don&#39;t save it</span>
</span><span class='line'>    <span class="n">matching_item</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collection</span><span class="o">.</span><span class="n">find_one</span><span class="p">(</span>
</span><span class='line'>        <span class="p">{</span><span class="s">&#39;session_id&#39;</span><span class="p">:</span> <span class="n">item</span><span class="p">[</span><span class="s">&#39;session_id&#39;</span><span class="p">],</span>
</span><span class='line'>         <span class="s">&#39;referring_url&#39;</span><span class="p">:</span> <span class="n">item</span><span class="p">[</span><span class="s">&#39;referring_url&#39;</span><span class="p">],</span>
</span><span class='line'>         <span class="s">&#39;current_url&#39;</span><span class="p">:</span> <span class="n">item</span><span class="p">[</span><span class="s">&#39;current_url&#39;</span><span class="p">]}</span>
</span><span class='line'>    <span class="p">)</span>
</span><span class='line'>    <span class="k">if</span> <span class="n">matching_item</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
</span><span class='line'>        <span class="k">raise</span> <span class="n">DropItem</span><span class="p">(</span>
</span><span class='line'>            <span class="s">&quot;Duplicate found for </span><span class="si">%s</span><span class="s">, </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span>
</span><span class='line'>            <span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s">&#39;referring_url&#39;</span><span class="p">],</span> <span class="n">item</span><span class="p">[</span><span class="s">&#39;current_url&#39;</span><span class="p">])</span>
</span><span class='line'>        <span class="p">)</span>
</span><span class='line'>    <span class="k">else</span><span class="p">:</span>
</span><span class='line'>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">insert_item</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">)</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>Lines 9-22 are a duplication of the parent class MongoDBPipeline.process_item() method to maintain compatability with the Scrapy-MongoDB configuration options. Line 25 is where we retrieve an entry that matches the specified fields. If an entry is found, then the current item is dropped and we return to crawling. The settings are:</p>

<p><figure class='code'><figcaption><span>settings.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">BOT_NAME</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">lsquo</span><span class="p">;</span><span class="n">farm2</span><span class="o">&amp;</span><span class="n">rsquo</span><span class="p">;</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">SPIDER_MODULES</span> <span class="o">=</span> <span class="p">[</span><span class="o">&amp;</span><span class="n">lsquo</span><span class="p">;</span><span class="n">farm2</span><span class="o">.</span><span class="n">spiders</span><span class="o">&amp;</span><span class="n">rsquo</span><span class="p">;]</span>
</span><span class='line'><span class="n">NEWSPIDER_MODULE</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">lsquo</span><span class="p">;</span><span class="n">farm2</span><span class="o">.</span><span class="n">spiders</span><span class="o">&amp;</span><span class="n">rsquo</span><span class="p">;</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">h1</span><span class="o">&gt;</span><span class="nb">set</span> <span class="n">to</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">no</span> <span class="n">depth</span> <span class="n">limit</span><span class="o">&lt;/</span><span class="n">h1</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">DEPTH_LIMIT</span> <span class="o">=</span> <span class="mi">3</span>
</span><span class='line'><span class="n">DOWNLOAD_DELAY</span> <span class="o">=</span> <span class="mi">2</span>
</span><span class='line'><span class="n">CONCURRENT_REQUESTS_PER_DOMAIN</span> <span class="o">=</span> <span class="mi">4</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span>
</span><span class='line'>    <span class="o">&amp;</span><span class="n">lsquo</span><span class="p">;</span><span class="n">farm2</span><span class="o">.</span><span class="n">pipelines</span><span class="o">.</span><span class="n">CustomMongoDBPipeline</span><span class="o">&amp;</span><span class="n">rsquo</span><span class="p">;:</span> <span class="mi">100</span>
</span><span class='line'><span class="p">}</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">h1</span><span class="o">&gt;&amp;</span><span class="n">lsquo</span><span class="p">;</span><span class="n">scrapy_mongodb</span><span class="o">.</span><span class="n">MongoDBPipeline</span><span class="o">&amp;</span><span class="n">rsquo</span><span class="p">;</span> <span class="n">connection</span><span class="o">&lt;/</span><span class="n">h1</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">MONGODB_URI</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">lsquo</span><span class="p">;</span><span class="n">mongodb</span><span class="p">:</span><span class="o">//</span><span class="n">localhost</span><span class="p">:</span><span class="mi">27017</span><span class="o">&amp;</span><span class="n">rsquo</span><span class="p">;</span>
</span><span class='line'><span class="n">MONGODB_DATABASE</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">lsquo</span><span class="p">;</span><span class="n">scrapy</span><span class="o">&amp;</span><span class="n">rsquo</span><span class="p">;</span>
</span><span class='line'><span class="n">MONGODB_COLLECTION</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">lsquo</span><span class="p">;</span><span class="n">items</span><span class="o">&amp;</span><span class="n">rsquo</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>Recall you can set the session_id from the command line</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scrapy crawl farm2 -a session_id=337</span></code></pre></td></tr></table></div></figure></p>
]]></content>
  </entry>
  
</feed>
