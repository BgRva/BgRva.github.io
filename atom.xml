<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[BgRva]]></title>
  <link href="http://BgRva.github.io/atom.xml" rel="self"/>
  <link href="http://BgRva.github.io/"/>
  <updated>2014-03-25T18:04:09-07:00</updated>
  <id>http://BgRva.github.io/</id>
  <author>
    <name><![CDATA[Michael Pastore]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Scraping to MongoDB]]></title>
    <link href="http://BgRva.github.io/blog/2014/03/23/scraping-to-mongodb/"/>
    <updated>2014-03-23T13:17:45-07:00</updated>
    <id>http://BgRva.github.io/blog/2014/03/23/scraping-to-mongodb</id>
    <content type="html"><![CDATA[<p>Now that we can crawl some sites (based on the previous posts), we need to persist the data somewhere where it can be retrieved and processed for analysis.  Recall that the goal of the scraper was to harvest connections between domains in order to generate a model of a specific industryâ€™s on-line presence.  In Scrapy, <a href="http://doc.scrapy.org/en/latest/topics/item-pipeline.html">Item Pipelines</a> are the prescribed mechanism by which scraped items can be persisted to a data store.  In this post, we will be using a custom pipeline extension for <a href="http://www.mongodb.org/">MongoDB</a> which we will further customize to check for duplicates.  The code examples in this post build upon those in the previous examples.  It is also assumed that you have some familiarity with MongoDB and have a working (basic) crawl spider.  It does not cover installing Scrapy, MongoDB, or any dependencies, (there is plenty of good documentation on these subjects).  Before continuing, you will need to install the following (in addition to having Scrapy working):</p>

<p> &ndash;<a href="http://www.mongodb.org/">MongoDB</a><br />
 &ndash;<a href="http://github.com/sebdah/scrapy-mongodb">Scrapy-MongoDB</a>, an item pipeline extension written by Sebastian Dahlgren</p>

<p>Once installed, the first step will be to get Scrapy-MongoDB working and saving to a collection.  Once you&rsquo;ve got MongoDB installed, create a database named <em>scrapy</em> and within it, a collection named <em>items</em>.  You can use the crawl spider from the previous posts and update the settings.py file to use Scrapy-MongoDB.  A quick note about MongoDB ids:  mongo will automatically add a unique id object with each item if and id field is not specified.  We will use default id and benefit from it because the default MongoDB id object contains an embedded <a href="http://docs.mongodb.org/manual/reference/method/ObjectId.getTimestamp/#ObjectId.getTimestamp">date-time stamp</a>.  Once you get Scrapy-MongoDB working and you are saving to a collection, we need to extend the <a href="https://github.com/sebdah/scrapy-mongodb/blob/master/scrapy_mongodb.py">MongoDBPipeline</a> class to include behavior to check for duplicates.  A duplicate of the example item is one in which the following fields match:</p>

<p> -Session_id<br/>
 -Current_url<br/>
 -Referring_url<br/></p>

<p>The <em>session_id</em> is used to group items from different harvests.  We want to persist a single connection between two given urls for each harvest session.  The <em>current_url</em> and <em>referring_url</em> will be used to represent the connection between two domains and will be used to generate a directed graph for the model, (a connection between two valid business backed domains can be used to infer a relationship which in turn can be used in social network analysis, more on this later &hellip;).</p>

<p>The new class will be called <em>CustomMongoDBPipeline</em> and should be placed within the <em>pipelines.py</em> file in the scrapy project folder.  You can keep the default pipeline initialized with the project in the same file and switch back by changing the settings file.</p>

<figure class='code'><figcaption><span>pipelines.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">scrapy.exceptions</span> <span class="kn">import</span> <span class="n">DropItem</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">scrapy_mongodb</span> <span class="kn">import</span> <span class="n">MongoDBPipeline</span>
</span><span class='line'>
</span><span class='line'><span class="k">class</span> <span class="nc">CustomMongoDBPipeline</span><span class="p">(</span><span class="n">MongoDBPipeline</span><span class="p">):</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
</span><span class='line'>        <span class="c"># the following lines are a duplication of MongoDBPipeline.process_item()</span>
</span><span class='line'>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s">&#39;buffer&#39;</span><span class="p">]:</span>
</span><span class='line'>            <span class="bp">self</span><span class="o">.</span><span class="n">current_item</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class='line'>            <span class="n">item</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s">&#39;append_timestamp&#39;</span><span class="p">]:</span>
</span><span class='line'>                <span class="n">item</span><span class="p">[</span><span class="s">&#39;scrapy-mongodb&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span> <span class="s">&#39;ts&#39;</span><span class="p">:</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">utcnow</span><span class="p">()</span> <span class="p">}</span>
</span><span class='line'>
</span><span class='line'>            <span class="bp">self</span><span class="o">.</span><span class="n">item_buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_item</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s">&#39;buffer&#39;</span><span class="p">]:</span>
</span><span class='line'>                <span class="bp">self</span><span class="o">.</span><span class="n">current_item</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class='line'>                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">insert_item</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">item_buffer</span><span class="p">,</span> <span class="n">spider</span><span class="p">)</span>
</span><span class='line'>            <span class="k">else</span><span class="p">:</span>
</span><span class='line'>                <span class="k">return</span> <span class="n">item</span>
</span><span class='line'>
</span><span class='line'>        <span class="c"># if the connection exists, don&#39;t save it</span>
</span><span class='line'>        <span class="n">matching_item</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collection</span><span class="o">.</span><span class="n">find_one</span><span class="p">(</span>
</span><span class='line'>            <span class="p">{</span><span class="s">&#39;session_id&#39;</span><span class="p">:</span> <span class="n">item</span><span class="p">[</span><span class="s">&#39;session_id&#39;</span><span class="p">],</span>
</span><span class='line'>             <span class="s">&#39;referring_url&#39;</span><span class="p">:</span> <span class="n">item</span><span class="p">[</span><span class="s">&#39;referring_url&#39;</span><span class="p">],</span>
</span><span class='line'>             <span class="s">&#39;current_url&#39;</span><span class="p">:</span> <span class="n">item</span><span class="p">[</span><span class="s">&#39;current_url&#39;</span><span class="p">]}</span>
</span><span class='line'>        <span class="p">)</span>
</span><span class='line'>        <span class="k">if</span> <span class="n">matching_item</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
</span><span class='line'>            <span class="k">raise</span> <span class="n">DropItem</span><span class="p">(</span>
</span><span class='line'>                <span class="s">&quot;Duplicate found for </span><span class="si">%s</span><span class="s">, </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span>
</span><span class='line'>                <span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s">&#39;referring_url&#39;</span><span class="p">],</span> <span class="n">item</span><span class="p">[</span><span class="s">&#39;current_url&#39;</span><span class="p">])</span>
</span><span class='line'>            <span class="p">)</span>
</span><span class='line'>        <span class="k">else</span><span class="p">:</span>
</span><span class='line'>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">insert_item</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>Lines 9-22 are a duplication of the parent class MongoDBPipeline.process_item() method to maintain compatability with the Scrapy-MongoDB configuration options.  Line 25 is where we retrieve an entry that matches the specified fields.  If an entry is found, then the current item is dropped and we return to crawling.  The settings are:</p>

<figure class='code'><figcaption><span>settings.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">BOT_NAME</span> <span class="o">=</span> <span class="s">&#39;farm2&#39;</span>
</span><span class='line'>
</span><span class='line'><span class="n">SPIDER_MODULES</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;farm2.spiders&#39;</span><span class="p">]</span>
</span><span class='line'><span class="n">NEWSPIDER_MODULE</span> <span class="o">=</span> <span class="s">&#39;farm2.spiders&#39;</span>
</span><span class='line'>
</span><span class='line'><span class="c">#set to 0 for no depth limit</span>
</span><span class='line'><span class="n">DEPTH_LIMIT</span> <span class="o">=</span> <span class="mi">3</span>
</span><span class='line'><span class="n">DOWNLOAD_DELAY</span> <span class="o">=</span> <span class="mi">2</span>
</span><span class='line'><span class="n">CONCURRENT_REQUESTS_PER_DOMAIN</span> <span class="o">=</span> <span class="mi">4</span>
</span><span class='line'>
</span><span class='line'><span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span>
</span><span class='line'>    <span class="s">&#39;farm2.pipelines.CustomMongoDBPipeline&#39;</span><span class="p">:</span> <span class="mi">100</span>
</span><span class='line'><span class="p">}</span>
</span><span class='line'>
</span><span class='line'><span class="c"># &#39;scrapy_mongodb.MongoDBPipeline&#39; connection</span>
</span><span class='line'><span class="n">MONGODB_URI</span> <span class="o">=</span> <span class="s">&#39;mongodb://localhost:27017&#39;</span>
</span><span class='line'><span class="n">MONGODB_DATABASE</span> <span class="o">=</span> <span class="s">&#39;scrapy&#39;</span>
</span><span class='line'><span class="n">MONGODB_COLLECTION</span> <span class="o">=</span> <span class="s">&#39;items&#39;</span>
</span></code></pre></td></tr></table></div></figure>


<p>Recall you can set the session_id from the command line</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">scrapy</span> <span class="n">crawl</span> <span class="n">farm2</span> <span class="o">-</span><span class="n">a</span> <span class="n">session_id</span><span class="o">=</span><span class="mi">337</span>
</span></code></pre></td></tr></table></div></figure>


<p>Next: Stopping Criteria</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scrapy After the Tutorials Part 2]]></title>
    <link href="http://BgRva.github.io/blog/2014/03/08/scrapy-after-tutorials-part-2/"/>
    <updated>2014-03-08T09:23:51-08:00</updated>
    <id>http://BgRva.github.io/blog/2014/03/08/scrapy-after-tutorials-part-2</id>
    <content type="html"><![CDATA[<p>As described in the previous post, the scraper will be crawling over extended periods of time.  Filtering out links which do not need to be crawled is one way of improving performance: less links to crawl = less items to process.  In this example, I hope to demonstrate how to filter the links to be crawled.  So how does the scraper determine which links to follow and how can we modify this behavior?  Using step #7 in the <a href="http://doc.scrapy.org/en/latest/topics/architecture.html#data-flow">Data Flow</a> as reference:</p>

<blockquote><p>The Spider processes the Response and returns scraped Items
and new Requests (to follow) to the Engine.</p></blockquote>

<p>So when a response is sent to the spider for processing, the spider will first extract all links in the  response and send them to the engine to be scheduled for crawling.  The spider <a href="http://doc.scrapy.org/en/latest/topics/spiders.html?highlight=rule#crawling-rules">rules</a> control which links will be passed to the engine for crawling.  Each rule uses a <a href="">link extractor</a> with which you can specify the links to follow or drop using the available parameters.  Many of the basic <a href="http://doc.scrapy.org/en/latest/topics/spiders.html?highlight=rule#crawlspider-example">examples</a> of crawl spiders demonstrate the &lsquo;allow&rsquo; and &lsquo;deny&rsquo; parameters for the rules.  But for this example, we need a bit more functionality.</p>

<p>By default, for each response processed by the spider, the set of urls send to the engine for crawling will include every href tag in the response.  To limit the amount of links that will be followed, we can remove the following types of links:</p>

<p> -any urls to the same domain as the response.url<br />
 -any relative urls</p>

<p>To remove urls to the same domain, we will implement a <em>filter_links()</em> for the crawling Rule.  To remove relative urls, we will add a regex to the link extractor to limit all urls returned to fully qualified urls, (e.g. start with <a href="http:">http:</a> or <a href="https:">https:</a>).  The Item has not changed from the last post.</p>

<figure class='code'><figcaption><span>Spider </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">from</span> <span class="nn">scrapy.contrib.spiders</span> <span class="kn">import</span> <span class="n">CrawlSpider</span><span class="p">,</span> <span class="n">Rule</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">scrapy.contrib.linkextractors.sgml</span> <span class="kn">import</span> <span class="n">SgmlLinkExtractor</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">scrapy.selector</span> <span class="kn">import</span> <span class="n">Selector</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">urlparse</span> <span class="kn">import</span> <span class="n">urlparse</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">farm1.items</span> <span class="kn">import</span> <span class="n">Farm1Item</span>
</span><span class='line'>
</span><span class='line'><span class="k">class</span> <span class="nc">Harvester2</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>
</span><span class='line'>    <span class="n">name</span> <span class="o">=</span> <span class="s">&#39;Harvester2&#39;</span>
</span><span class='line'>    <span class="n">session_id</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
</span><span class='line'>    <span class="n">response_url</span> <span class="o">=</span> <span class="s">&quot;&quot;</span>
</span><span class='line'>    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">&quot;http://www.mmorpg.com&quot;</span><span class="p">]</span>
</span><span class='line'>    <span class="n">rules</span> <span class="o">=</span> <span class="p">(</span>
</span><span class='line'>        <span class="n">Rule</span> <span class="p">(</span>
</span><span class='line'>            <span class="n">SgmlLinkExtractor</span><span class="p">(</span>
</span><span class='line'>                <span class="n">allow</span><span class="o">=</span><span class="p">(</span><span class="s">&quot;((mailto\:|(news|(ht|f)tp(s?))\://){1}\S+)&quot;</span><span class="p">,</span> <span class="p">),),</span>
</span><span class='line'>            <span class="n">callback</span><span class="o">=</span><span class="s">&quot;parse_items&quot;</span><span class="p">,</span>
</span><span class='line'>            <span class="n">process_links</span><span class="o">=</span><span class="s">&quot;filter_links&quot;</span><span class="p">,</span>
</span><span class='line'>            <span class="n">follow</span><span class="o">=</span> <span class="bp">True</span><span class="p">),</span>
</span><span class='line'>    <span class="p">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">session_id</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span><span class='line'>        <span class="nb">super</span><span class="p">(</span><span class="n">Harvester2</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span class='line'>        <span class="bp">self</span><span class="o">.</span><span class="n">session_id</span> <span class="o">=</span> <span class="n">session_id</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">def</span> <span class="nf">parse_start_url</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
</span><span class='line'>        <span class="bp">self</span><span class="o">.</span><span class="n">response_url</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span>
</span><span class='line'>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse_items</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">def</span> <span class="nf">parse_items</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
</span><span class='line'>        <span class="bp">self</span><span class="o">.</span><span class="n">response_url</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span>
</span><span class='line'>        <span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</span><span class='line'>        <span class="n">items</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class='line'>        <span class="n">item</span> <span class="o">=</span> <span class="n">Farm1Item</span><span class="p">()</span>
</span><span class='line'>        <span class="n">item</span><span class="p">[</span><span class="s">&quot;session_id&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">session_id</span>
</span><span class='line'>        <span class="n">item</span><span class="p">[</span><span class="s">&quot;depth&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s">&quot;depth&quot;</span><span class="p">]</span>
</span><span class='line'>        <span class="n">item</span><span class="p">[</span><span class="s">&quot;title&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//title/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</span><span class='line'>        <span class="n">item</span><span class="p">[</span><span class="s">&quot;current_url&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span>
</span><span class='line'>        <span class="n">referring_url</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">headers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&#39;Referer&#39;</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
</span><span class='line'>        <span class="n">item</span><span class="p">[</span><span class="s">&quot;referring_url&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">referring_url</span>
</span><span class='line'>        <span class="n">items</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
</span><span class='line'>        <span class="k">return</span> <span class="n">items</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">def</span> <span class="nf">filter_links</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">links</span><span class="p">):</span>
</span><span class='line'>        <span class="n">baseDomain</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_base_domain</span><span class="p">(</span> <span class="bp">self</span><span class="o">.</span><span class="n">response_url</span><span class="p">)</span>
</span><span class='line'>        <span class="n">filteredLinks</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class='line'>        <span class="k">for</span> <span class="n">link</span> <span class="ow">in</span> <span class="n">links</span><span class="p">:</span>
</span><span class='line'>            <span class="k">if</span> <span class="n">link</span><span class="o">.</span><span class="n">url</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">baseDomain</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span class='line'>                <span class="n">filteredLinks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">link</span><span class="p">)</span>
</span><span class='line'>        <span class="k">return</span> <span class="n">filteredLinks</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">def</span> <span class="nf">get_base_domain</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">url</span><span class="p">):</span>
</span><span class='line'>        <span class="n">base</span> <span class="o">=</span> <span class="n">urlparse</span><span class="p">(</span><span class="n">url</span><span class="p">)</span><span class="o">.</span><span class="n">netloc</span>
</span><span class='line'>        <span class="k">if</span> <span class="n">base</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s">&quot;WWW.&quot;</span><span class="p">):</span>
</span><span class='line'>            <span class="n">base</span> <span class="o">=</span> <span class="n">base</span><span class="p">[</span><span class="mi">4</span><span class="p">:]</span>
</span><span class='line'>        <span class="k">elif</span> <span class="n">base</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s">&quot;FTP.&quot;</span><span class="p">):</span>
</span><span class='line'>            <span class="n">base</span> <span class="o">=</span> <span class="n">base</span><span class="p">[</span><span class="mi">4</span><span class="p">:]</span>
</span><span class='line'>        <span class="c"># drop any ports</span>
</span><span class='line'>        <span class="n">base</span> <span class="o">=</span> <span class="n">base</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&#39;:&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span><span class='line'><span class="k">return</span> <span class="n">base</span>
</span></code></pre></td></tr></table></div></figure>


<p>Note:  you can add this spider to the previous project by dropping it in the same folder as the other spider.  As long as the file name and spider name are different, it will run with the same item and settings.</p>

<p>The <em>response_url</em> class variable (line 10) will maintain the current response.url and will be needed for filtering urls.</p>

<p>The <a href="http://doc.scrapy.org/en/latest/topics/spiders.html?highlight=rule#scrapy.contrib.spiders.CrawlSpider.parse_start_url">parse_start_url()</a> method (line 25) overrides the base definition and is called only for the defined <em>start_urls</em> and processes each start url as an item.  Most importantly, it initializes the <em>response_url</em> variable before any crawling.</p>

<p>The <em>parse_item()</em> method has not changed from the last example.</p>

<p>The <em>get_base_domain()</em> method (line 51) returns the base domain for a url.  For example, if &lsquo;http ://www.b.com:334/x/y.htm?z&rsquo; is passed in, &lsquo;b.com&rsquo; will be returned.</p>

<p>The <em>filter_links()</em> method will filter all links passed in and only return links to external domains.  The base domain of the <em>response_url</em> is retrieved by <em>get_base_domain()</em> method (line 44).  Any links in the input list of links which contain this base domain are filtered out.  Only links to different (external) domains are returned.  This method is assigned to the <em>process_links</em> parameter in the Rule (line line 40).</p>

<p>The <em>allow</em> parameter for the link extractor (line 15) is assigned the regular expression &ldquo;((mailto\:|(news|(ht|f)tp(s?))\://){1}\S+)&rdquo; which precludes any relative urls by only allowing urls that start with a <a href="http://en.wikipedia.org/wiki/URI_scheme">URI Scheme</a> (e.g. ftp, http, etc.).</p>

<p>When a response is sent to a spider for processing, the <em>filter_links()</em> method is called before the <em>process_item()</em> method.  In this example, if the <em>response_url</em> variable is not set, it will fail.  This is the reason we needed to use the <em>parse_start_url()</em> method, which is called for each defined <em>start_url</em> and allows us to set the <em>response_url</em> variable before any links are processed by <em>filter_links()</em>.  This only allows fully qualified links to external urls to be sent to the engine for scheduled crawling.</p>

<p>Next: Some MongoDB</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scrapy After the Tutorials Part 1]]></title>
    <link href="http://BgRva.github.io/blog/2014/03/04/scrapy-after-tutorials-part-1/"/>
    <updated>2014-03-04T17:27:39-08:00</updated>
    <id>http://BgRva.github.io/blog/2014/03/04/scrapy-after-tutorials-part-1</id>
    <content type="html"><![CDATA[<p>I was given the task of building a scalable web scraper to harvest connections between domains of a specific industry in order to generate a network model of that industryâ€™s online presence.  The scraper will need to run for a period of time, (say a week), and the resulting harvest would be the raw data from which the model would be generated.  This harvesting process will need to be repeatable to create â€˜snapshotsâ€™ of the network for future longitudinal analysis.  The implementation will use <a href="http://www.yahoo.com" title="Scrapy">scrapy</a> with a <a href="http://www.mongodb.org" title="MongoDB">MongoDB</a> back end on a linux platform to be run (ultimately) in AWS.</p>

<p>This post (and subsequent ones) will provide some code samples and documentation of issues faced during the process of getting the scraper operational.  The intent is to help bridge the gap between the initial scrapy tutorials and real-world code.  The examples assume you have scrapy installed and running, and have at least worked through the basic tutorials.  I did find many of the <a href="https://github.com/scrapy/scrapy/wiki">tutorials</a> on the wiki very helpful and worked through several of them, (multiple times).  Get a basic crawl spider up and running and then pick up here.</p>

<p>In this example, I hope to demonstrate the the following scrapy features:<br />
  &ndash; Adding a spider parameter and using it from the command line<br />
  &ndash; Getting current crawl depth and the referring url<br />
  &ndash; Setting crawl depth limits</p>

<p>The code samples below were from a scrapy project named <strong>farm1</strong></p>

<p>The item:</p>

<figure class='code'><figcaption><span>Item </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">from</span> <span class="nn">scrapy.item</span> <span class="kn">import</span> <span class="n">Item</span><span class="p">,</span> <span class="n">Field</span>
</span><span class='line'>
</span><span class='line'><span class="k">class</span> <span class="nc">Farm1Item</span><span class="p">(</span><span class="n">Item</span><span class="p">):</span>
</span><span class='line'>    <span class="n">session_id</span> <span class="o">=</span> <span class="n">Field</span><span class="p">()</span>
</span><span class='line'>    <span class="n">depth</span> <span class="o">=</span> <span class="n">Field</span><span class="p">()</span>
</span><span class='line'>    <span class="n">current_url</span> <span class="o">=</span> <span class="n">Field</span><span class="p">()</span>
</span><span class='line'>    <span class="n">referring_url</span> <span class="o">=</span> <span class="n">Field</span><span class="p">()</span>
</span><span class='line'>    <span class="n">title</span> <span class="o">=</span> <span class="n">Field</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure>


<p><em>session_id</em>:  a unique session id for each scrapy run or harvest<br />
<em>depth</em>:  the depth of the current page with respect to the start url<br />
<em>current_url</em>: the url of the current page being processed<br />
<em>referring_url</em>: the url of the site which was linked to the current page<br /></p>

<p>The crawl spider:</p>

<figure class='code'><figcaption><span>Spider </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">from</span> <span class="nn">scrapy.contrib.spiders</span> <span class="kn">import</span> <span class="n">CrawlSpider</span><span class="p">,</span> <span class="n">Rule</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">scrapy.contrib.linkextractors.sgml</span> <span class="kn">import</span> <span class="n">SgmlLinkExtractor</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">scrapy.selector</span> <span class="kn">import</span> <span class="n">Selector</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">farm1.items</span> <span class="kn">import</span> <span class="n">Farm1Item</span>
</span><span class='line'>
</span><span class='line'><span class="k">class</span> <span class="nc">Harvester1</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>
</span><span class='line'>    <span class="n">name</span> <span class="o">=</span> <span class="s">&#39;Harvester1&#39;</span>
</span><span class='line'>    <span class="n">session_id</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
</span><span class='line'>    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">&quot;http://www.example.com&quot;</span><span class="p">]</span>
</span><span class='line'>    <span class="n">rules</span> <span class="o">=</span> <span class="p">(</span> <span class="n">Rule</span> <span class="p">(</span><span class="n">SgmlLinkExtractor</span><span class="p">(</span><span class="n">allow</span><span class="o">=</span><span class="p">(</span><span class="s">&quot;&quot;</span><span class="p">,</span> <span class="p">),),</span>
</span><span class='line'>                <span class="n">callback</span><span class="o">=</span><span class="s">&quot;parse_items&quot;</span><span class="p">,</span>  <span class="n">follow</span><span class="o">=</span> <span class="bp">True</span><span class="p">),</span>
</span><span class='line'>    <span class="p">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">session_id</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span><span class='line'>        <span class="nb">super</span><span class="p">(</span><span class="n">Harvester1</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span class='line'>        <span class="bp">self</span><span class="o">.</span><span class="n">session_id</span> <span class="o">=</span> <span class="n">session_id</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">def</span> <span class="nf">parse_items</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
</span><span class='line'>        <span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</span><span class='line'>        <span class="n">items</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class='line'>        <span class="n">item</span> <span class="o">=</span> <span class="n">Farm1Item</span><span class="p">()</span>
</span><span class='line'>        <span class="n">item</span><span class="p">[</span><span class="s">&quot;session_id&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">session_id</span>
</span><span class='line'>        <span class="n">item</span><span class="p">[</span><span class="s">&quot;depth&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s">&quot;depth&quot;</span><span class="p">]</span>
</span><span class='line'>        <span class="n">item</span><span class="p">[</span><span class="s">&quot;current_url&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span>
</span><span class='line'>        <span class="n">referring_url</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">headers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&#39;Referer&#39;</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
</span><span class='line'>        <span class="n">item</span><span class="p">[</span><span class="s">&quot;referring_url&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">referring_url</span>
</span><span class='line'>        <span class="n">item</span><span class="p">[</span><span class="s">&quot;title&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//title/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</span><span class='line'>        <span class="n">items</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
</span><span class='line'>        <span class="k">return</span> <span class="n">items</span>
</span></code></pre></td></tr></table></div></figure>


<p>The spider uses the <a href="http://doc.scrapy.org/en/latest/topics/link-extractors.html#sgmllinkextractor">SgmlLinkExtractor</a> and follows every link, (a later post will cover filtering which links to follow).</p>

<h4>Adding a spider parameter and using it from the command line</h4>

<p>Lines 14-16 in the spider shows the constructor which has a <em>session_id</em> parameter
with a defaul assignment.  The session id is assigned to the spider and persisted with each item processed and will be used to identify items from different harvest sessions.  Defining the parameter in the constructor allows using it from the command line:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>x:~$ scrapy crawl Harvester7 -a session_id=337</span></code></pre></td></tr></table></div></figure>


<h4>Getting current crawl depth and the referring url</h4>

<p>Line 23 is where the current depth of the crawl is retrieved.  The â€˜depthâ€™ key word is one of several
<a href="http://doc.scrapy.org/en/latest/topics/request-response.html?highlight=meta#request-meta-special-keys" title="pre-defined keys">predefined keys</a> in the meta dictionary for the request and response objects.  The depth value will be used in the analysis phase.</p>

<p>Line 25 shows how to retrieve the referring url.  The goal of this project was harvesting connections between domains, not just data from individual pages.  For each item processed, the connection between two links: <em>referring_url</em> &ndash;> <em>current_url</em> is stored.  Implied with each connection is the directionality of the link which will help build a directed graph for analysis.  This is enabled by default in the <a href="https://scrapy.readthedocs.org/en/latest/topics/settings.html#std:setting-SPIDER_MIDDLEWARES_BASE" title="default middleware settings">default middleware settings</a>, (and note that &lsquo;Referer&rsquo; is the correct usage).</p>

<h4>Setting crawl depth limits</h4>

<p>Rather than use ctrl-c to kill the crawling spider, you can set a depth limit at which the spider will not go beyond.  I found this helpful during testing.  DEPTH_LIMIT is a predefined setting that can be assigned in your settings file.  This was the only additional setting used in this example other than the defaults created with the project.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>DEPTH_LIMIT = 3</span></code></pre></td></tr></table></div></figure>


<p>The depth limit can also be set in the <a href="http://doc.scrapy.org/en/latest/topics/settings.html?highlight=depth#global-overrides">command line</a>, (as can all pre-defined settings):</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>x:~$ scrapy crawl Harvester1 â€“s DEPTH_LIMIT=2</span></code></pre></td></tr></table></div></figure>


<p>The command line assignment will take priority of the settings file.  Note that the depth limit will have little affect if you are running a <a href="http://doc.scrapy.org/en/latest/topics/broad-crawls.html?highlight=broad#broad-crawls">broad crawl</a>.  Ultimately, when this scraper is released into the wild, it will probably be set to run a broad crawl.  But it will still need to troll deep to really pull out much of a domain&rsquo;s public connections.  As to how this will be handled, (a mix of broad + deep crawls), I am not sure but it will be documented here once it is figured out.</p>

<p>Next:  Filtering links to follow</p>
]]></content>
  </entry>
  
</feed>
